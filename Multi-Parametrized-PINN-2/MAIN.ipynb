{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install orbingol::geomdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install conda-forge::opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install conda-forge::numpy-stl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "import os\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import mask\n",
    "import functional as fn\n",
    "import visualization as vis\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TYPE = 'train'\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "L1 = 0.0025\n",
    "L2 = 0.01\n",
    "H_2 = 0.0005\n",
    "\n",
    "MU = 3e-3\n",
    "RHO = 1050\n",
    "V_MAX = 0.3\n",
    "P_MAX = 90.\n",
    "Q = (V_MAX / 2) * 2 * H_2\n",
    "\n",
    "\n",
    "N = 257\n",
    "N_POINTS = 1000\n",
    "\n",
    "METHOD = 'PINN'\n",
    "USE_WEIGHT_OPT = False\n",
    "N_NEURONS = 48#32\n",
    "N_LAYERS = 15#10\n",
    "SCHEDULER_GAMMA = 0.99\n",
    "SCHEDULER_PATIENCE = 140\n",
    "BATCH_SIZE = 6 #3\n",
    "EPOCHS = 100000\n",
    "\n",
    "N_INTERIOR = 500\n",
    "N_WALLS = 250\n",
    "N_INLET = 100\n",
    "N_OUTLET = 100\n",
    "N_BOUNDARY = N_WALLS + N_INLET + N_OUTLET\n",
    "\n",
    "W_RES = 0.02\n",
    "W_DIV = 0.02\n",
    "W_V0 = 10000.\n",
    "W_V_INLET = 1000. #1000.\n",
    "W_P_OUTLET = 1.\n",
    "\n",
    "if USE_WEIGHT_OPT:\n",
    "    W_RES = torch.tensor(W_RES).to('cuda')\n",
    "    W_RES.requires_grad_(True)\n",
    "    W_DIV = torch.tensor(W_DIV).to('cuda')\n",
    "    W_DIV.requires_grad_(True)\n",
    "    W_V0 = torch.tensor(W_V0).to('cuda')\n",
    "    W_V0.requires_grad_(True)\n",
    "    W_V_INLET = torch.tensor(W_V_INLET).to('cuda')\n",
    "    W_V_INLET.requires_grad_(True)\n",
    "    W_P_OUTLET = torch.tensor(W_P_OUTLET).to('cuda')\n",
    "    W_P_OUTLET.requires_grad_(True)\n",
    "\n",
    "# NORM_IN_SUB = torch.tensor([0.75, 4., 0, 0]).reshape(1, 1, 4).to(DEVICE)\n",
    "# NORM_IN_DIV = torch.tensor([0.33333, 1., 0.5 * L1, 0.5 * L2]).reshape(1, 1, 4).to(DEVICE)\n",
    "NORM_OUT = torch.tensor([V_MAX, V_MAX, P_MAX]).reshape(1, 1, 3).to(DEVICE)\n",
    "\n",
    "vis.set_params(12, L1, L2, 5)\n",
    "\n",
    "LATENT_DIM = 3 #128\n",
    "FINE_TUNE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(x, n):\n",
    "    ind = torch.randperm(len(x))[:n]\n",
    "    return x[ind]\n",
    "\n",
    "\n",
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, path, encoder):\n",
    "        self.interior = []\n",
    "        self.walls = []\n",
    "        self.inlet = []\n",
    "        self.outlet = []\n",
    "        self.center = []\n",
    "        self.h = []\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        files = os.listdir(path)\n",
    "\n",
    "        # png_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "        # count = len(png_files)\n",
    "\n",
    "        self.masks = torch.zeros(len(files), 1, N, N).float()\n",
    "        self.embeding = torch.zeros(len(files), 3).float() #torch.zeros(len(files), 128).float() #torch.zeros(len(files), 2).float()\n",
    "\n",
    "        for idx in range(len(files)):\n",
    "            file_path = os.path.join(path, files[idx])\n",
    "            if '.stl' in file_path:\n",
    "                pass\n",
    "                # mask_, x_interior, x_walls, x_inlet, x_outlet, center, h = mask.load_stl(file_path, N,\n",
    "                #                                                             N_POINTS * N_INTERIOR,\n",
    "                #                                                             N_POINTS * N_WALLS,\n",
    "                #                                                             N_POINTS * N_INLET,\n",
    "                #                                                             N_POINTS * N_OUTLET,\n",
    "                #                                                             length=[L1, L1, L2],\n",
    "                #                                                             device='cuda',\n",
    "                #                                                             use_3d=False,\n",
    "                #                                                             inside_buffer= 0.001)\n",
    "                \n",
    "                # self.interior.append(x_interior)\n",
    "                # self.walls.append(x_walls)\n",
    "                # self.inlet.append(x_inlet)\n",
    "                # self.outlet.append(x_outlet)\n",
    "                # self.center.append(torch.tensor(center).to(DEVICE))\n",
    "                # self.h.append(torch.tensor(h).to(DEVICE))\n",
    "                \n",
    "                # self.masks[idx] = mask_['num']\n",
    "                # self.embeding[idx, 0] = float(files[idx].split('_')[1].replace('-', '.'))\n",
    "                # self.embeding[idx, 1] = float(files[idx].split('_')[3][0])\n",
    "            else:\n",
    "                mask_, x_interior, x_walls, x_inlet, x_outlet, center, h = mask.img_to_mask(file_path, N,\n",
    "                                                                            N_POINTS * N_INTERIOR,\n",
    "                                                                            N_POINTS * N_WALLS,\n",
    "                                                                            N_POINTS * N_INLET,\n",
    "                                                                            N_POINTS * N_OUTLET,\n",
    "                                                                            length=[L1, L1, L2],\n",
    "                                                                            device='cuda',\n",
    "                                                                            use_3d=False,\n",
    "                                                                            inside_buffer= 0.001)\n",
    "                \n",
    "                self.interior.append(x_interior)\n",
    "                self.walls.append(x_walls)\n",
    "                self.inlet.append(x_inlet)\n",
    "                self.outlet.append(x_outlet)\n",
    "                self.center.append(torch.tensor(center).to(DEVICE))\n",
    "                self.h.append(torch.tensor(h).to(DEVICE))\n",
    "                \n",
    "                self.masks[idx] = mask_['num']\n",
    "                \n",
    "                ## self.embeding[idx, 0] = float(files[idx].split('_')[1].replace('-', '.'))\n",
    "                ## self.embeding[idx, 1] = float(files[idx].split('_')[3][0])\n",
    "\n",
    "                # img = Image.open(file_path)\n",
    "                # img_array = np.array(img)\n",
    "                # img_tensor = torch.from_numpy(img_array / 255.0).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                #     self.embeding[idx] = self.encoder(img_tensor)\n",
    "\n",
    "                numbers = re.findall(r'idx_(\\d+)_(\\d+)_(\\d+)\\.png', files[idx])\n",
    "                self.embeding[idx] = torch.tensor([int(n) for n in numbers[0]])\n",
    "\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_interior = self.sampler(self.interior[idx], N_INTERIOR).to(DEVICE)\n",
    "\n",
    "        emb_interior = self.embeding[idx].repeat(N_INTERIOR, 1).to(DEVICE)\n",
    "        \n",
    "        inputs_boundary = torch.cat((emb_interior[0].repeat(N_BOUNDARY, 1),\n",
    "                                torch.cat((self.sampler(self.walls[idx], N_WALLS),\n",
    "                                           self.sampler(self.inlet[idx], N_INLET),\n",
    "                                           self.sampler(self.outlet[idx], N_OUTLET))).to(DEVICE)), 1)\n",
    "        return emb_interior, x_interior, inputs_boundary, self.masks[idx], self.center[idx], self.h[idx]\n",
    "        \n",
    "def sample_boundarys(x):\n",
    "    x_walls = x[:, :N_WALLS]\n",
    "    x_inlet = x[:, N_WALLS:N_WALLS + N_INLET]\n",
    "    x_outlet = x[:, N_WALLS + N_INLET:N_WALLS + N_INLET + N_OUTLET]\n",
    "\n",
    "    return x_walls, x_inlet, x_outlet\n",
    "\n",
    "\n",
    "def sample_v_inlet(inputs, center, h):\n",
    "    with torch.no_grad():\n",
    "        v_inlet_2 = (-((inputs[..., -2] - center) / - h) ** 2 + 1) * (2 * Q / (2 * h))\n",
    "        v_inlet_1 = torch.zeros_like(v_inlet_2)\n",
    "    return torch.stack((v_inlet_1, v_inlet_2), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN\n",
    "\n",
    "## CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder network for CAE.\n",
    "    Args:\n",
    "        latent_dim (int): Dimension of latent space representation\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # (1, 460, 648) -> (16, 230, 324)\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (16, 230, 324) -> (32, 115, 162)\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (32, 115, 162) -> (64, 58, 81)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (64, 58, 81) -> (128, 29, 41)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self._calculate_fc_input(latent_dim)\n",
    "    \n",
    "    def _calculate_fc_input(self, latent_dim):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 460, 648)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            self.fc_input_dim = dummy_output.numel() // dummy_output.shape[0]\n",
    "            self.fc = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Convolutional decoder network for CAE.\n",
    "    Args:\n",
    "        latent_dim (int): Dimension of latent space representation\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "  \n",
    "        self.fc_output_dim = 128 * 29 * 41\n",
    "        self.fc = nn.Linear(latent_dim, self.fc_output_dim)\n",
    "        \n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.Unflatten(1, (128, 29, 41)),\n",
    "            \n",
    "            # (128, 29, 41) -> (64, 58, 82)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (64, 58, 82) -> (32, 116, 164)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (32, 116, 164) -> (16, 232, 328)\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (16, 232, 328) -> (1, 460, 648)\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=5, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.deconv_layers(x)\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    \"\"\"Complete Convolutional Autoencoder architecture.\n",
    "    Args:\n",
    "        latent_dim (int): Dimension of latent space representation\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        \n",
    "        if reconstructed.shape[2:] != x.shape[2:]:\n",
    "            reconstructed = F.interpolate(reconstructed, size=x.shape[2:], mode='bilinear')\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cae_model = CAE(latent_dim=128).to(DEVICE)\n",
    "cae_model.load_state_dict(torch.load(\"models_cae/best_val_cae.pth\"))\n",
    "encoder = cae_model.encoder\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.findall(r'idx_(\\d+)_(\\d+)_(\\d+)\\.png', 'idx_1_1_50.png')\n",
    "embedding = np.array([int(n) for n in numbers[0]])\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_latent_stats(dataset_path, encoder, device='cuda'):\n",
    "    encoder.eval()\n",
    "    latent_vectors = []\n",
    "    \n",
    "    files = os.listdir(dataset_path)\n",
    "\n",
    "    for idx in range(len(files)):\n",
    "        file_path = os.path.join(dataset_path, files[idx])\n",
    "        if '.png' in file_path:\n",
    "\n",
    "            # img = Image.open(file_path)\n",
    "            # img_array = np.array(img)\n",
    "            # img_tensor = torch.from_numpy(img_array / 255.0).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     embedding = encoder(img_tensor).cpu().numpy()\n",
    "            #     latent_vectors.append(embedding)\n",
    "            \n",
    "            numbers = re.findall(r'idx_(\\d+)_(\\d+)_(\\d+)\\.png', files[idx])\n",
    "            embedding = np.array([int(n) for n in numbers[0]])\n",
    "            latent_vectors.append(embedding)\n",
    "\n",
    "    latent_array = np.vstack(latent_vectors)\n",
    "    #latent_array = np.concatenate(latent_vectors, axis=0)\n",
    "    mean = latent_array.mean(axis=0).astype(np.float32)\n",
    "    std = latent_array.std(axis=0).astype(np.float32)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mean, emb_std = calculate_latent_stats('dataset/mini_train',encoder)\n",
    "print(f'mean = {emb_mean}, \\nstd = {emb_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_IN_SUB = torch.tensor(np.concatenate([emb_mean, np.array([0, 0])])).float().reshape(1, 1, LATENT_DIM+2).to(DEVICE)\n",
    "NORM_IN_DIV = torch.tensor(np.concatenate([emb_std + 1e-8, np.array([0.5 * L1, 0.5 * L2])])).float().reshape(1, 1, LATENT_DIM+2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(input_size, hidden_layer_size, hidden_layers, output_size):\n",
    "    layers = []\n",
    "    for i in range(hidden_layers):\n",
    "        if not i:\n",
    "            layers += [nn.Linear(input_size, hidden_layer_size, bias=True),\n",
    "                       nn.SiLU(inplace=True)]\n",
    "            torch.nn.init.xavier_normal_(layers[-2].weight)\n",
    "        else:\n",
    "            layers += [nn.Linear(hidden_layer_size, hidden_layer_size, bias=True),\n",
    "                       nn.SiLU(inplace=True)]\n",
    "            torch.nn.init.xavier_normal_(layers[-2].weight)\n",
    "            \n",
    "    layers.append(nn.Linear(hidden_layer_size, output_size, bias=True))\n",
    "    torch.nn.init.xavier_normal_(layers[-1].weight)\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hyp):\n",
    "        super(MLP, self).__init__()\n",
    "        input_size = 5 #130 # 4\n",
    "        \n",
    "        output_size = 3 if METHOD == 'PINN' else 2\n",
    "        self.mlp = []\n",
    "        for i in range(output_size):\n",
    "            layers = create_layers(input_size, hyp['hidden_layer_size'], hyp['hidden_layers'], 1)\n",
    "            self.mlp.append(nn.Sequential(*layers))\n",
    "        self.mlp1, self.mlp2, self.mlp3 = self.mlp\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = []\n",
    "        for i in range(self.output_size):\n",
    "            y.append(self.mlp[i]((x - NORM_IN_SUB) / NORM_IN_DIV) * NORM_OUT[..., i])\n",
    "        return torch.cat(y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassDataset('dataset/mini_train', encoder)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m losses[\u001b[33m'\u001b[39m\u001b[33mPINN_loss\u001b[39m\u001b[33m'\u001b[39m].append(\u001b[32m0.\u001b[39m)\n\u001b[32m     40\u001b[39m losses[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m].append(\u001b[32m0.\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43memb_interior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_interior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_interior\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mClassDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     88\u001b[39m x_interior = \u001b[38;5;28mself\u001b[39m.sampler(\u001b[38;5;28mself\u001b[39m.interior[idx], N_INTERIOR).to(DEVICE)\n\u001b[32m     90\u001b[39m emb_interior = \u001b[38;5;28mself\u001b[39m.embeding[idx].repeat(N_INTERIOR, \u001b[32m1\u001b[39m).to(DEVICE)\n\u001b[32m     92\u001b[39m inputs_boundary = torch.cat((emb_interior[\u001b[32m0\u001b[39m].repeat(N_BOUNDARY, \u001b[32m1\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m                         torch.cat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwalls\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_WALLS\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     94\u001b[39m                                    \u001b[38;5;28mself\u001b[39m.sampler(\u001b[38;5;28mself\u001b[39m.inlet[idx], N_INLET),\n\u001b[32m     95\u001b[39m                                    \u001b[38;5;28mself\u001b[39m.sampler(\u001b[38;5;28mself\u001b[39m.outlet[idx], N_OUTLET))).to(DEVICE)), \u001b[32m1\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m emb_interior, x_interior, inputs_boundary, \u001b[38;5;28mself\u001b[39m.masks[idx], \u001b[38;5;28mself\u001b[39m.center[idx], \u001b[38;5;28mself\u001b[39m.h[idx]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36msampler\u001b[39m\u001b[34m(x, n)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msampler\u001b[39m(x, n):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     ind = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:n]\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x[ind]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if RUN_TYPE == 'train':\n",
    "    best_loss = 1e10\n",
    "\n",
    "    hyp = {'hidden_layer_size': N_NEURONS, 'hidden_layers': N_LAYERS}\n",
    "    if FINE_TUNE:\n",
    "        model = MLP(hyp).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(f'work/{METHOD}_{SCHEDULER_PATIENCE}.pth'))\n",
    "        with open(f'work/{METHOD}_history_{SCHEDULER_PATIENCE}.json', 'r') as fp:\n",
    "            losses = json.load(fp)\n",
    "    else: \n",
    "        model = MLP(hyp).to(DEVICE)\n",
    "        losses = {'residual_loss': [],\n",
    "              'div(v)_loss': [],\n",
    "              'ns_loss': [],\n",
    "              'v_inlet_loss': [],\n",
    "              'p_outlet_loss': [],\n",
    "              'PINN_loss': [],\n",
    "              'lr': []}\n",
    "\n",
    "    if USE_WEIGHT_OPT:\n",
    "        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 1e-3, 'weight_decay': 0.},\n",
    "                                    {'params': [W_RES, W_DIV, W_V0, W_V_INLET, W_P_OUTLET], 'lr': 1e-3, 'weight_decay': 0.}],\n",
    "                                    lr=1e-3, weight_decay=0.)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"\\rEpoch: {i+1}\", end=\"\", flush=True)\n",
    "        n_iter = 0.\n",
    "\n",
    "        losses['residual_loss'].append(0.)\n",
    "        losses['div(v)_loss'].append(0.)\n",
    "        losses['ns_loss'].append(0.)\n",
    "        losses['v_inlet_loss'].append(0.)\n",
    "        losses['p_outlet_loss'].append(0.)\n",
    "        losses['PINN_loss'].append(0.)\n",
    "        losses['lr'].append(0.)\n",
    "\n",
    "        for emb_interior, x_interior, inputs_boundary, mask_, center, h in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_interior.requires_grad_(True)\n",
    "\n",
    "            inputs_interior = torch.cat((emb_interior,\n",
    "                                        x_interior), 2)\n",
    "            \n",
    "            outputs_interior = model(inputs_interior)\n",
    "\n",
    "            outputs_walls, outputs_inlet, outputs_outlet = sample_boundarys(model(inputs_boundary))\n",
    "\n",
    "            dv1, dv2, d2v1, d2v2, dp = fn.calc_grad(outputs_interior, x_interior)\n",
    "            \n",
    "            res_loss = fn.zero_loss(fn.calc_res(outputs_interior, dv1, dv2, d2v1, d2v2, dp, MU, RHO))\n",
    "            div_loss = fn.mse_zero_loss(fn.calc_div(dv1, dv2))\n",
    "            v0_loss = fn.mse_zero_loss(outputs_walls[..., :2])\n",
    "            v_inlet_loss = torch.nn.functional.mse_loss(outputs_inlet[..., :2], sample_v_inlet(sample_boundarys(inputs_boundary)[1],\n",
    "                                                                                               center.unsqueeze(1),\n",
    "                                                                                               h.unsqueeze(1)))\n",
    "            p_outlet_loss = fn.mse_zero_loss(outputs_outlet[..., 2])\n",
    "\n",
    "            loss = W_RES * res_loss + W_DIV * div_loss + W_V0 * v0_loss + W_V_INLET * v_inlet_loss + W_P_OUTLET * p_outlet_loss\n",
    "            loss.backward()\n",
    "\n",
    "            losses['residual_loss'][-1] += (res_loss.item())\n",
    "            losses['div(v)_loss'][-1] += (div_loss.item())\n",
    "            losses['ns_loss'][-1] += (v0_loss.item())\n",
    "            losses['v_inlet_loss'][-1] += (v_inlet_loss.item())\n",
    "            losses['p_outlet_loss'][-1] += (p_outlet_loss.item())\n",
    "            losses['PINN_loss'][-1] += (loss.item())\n",
    "            losses['lr'][-1] += (lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        losses['residual_loss'][-1] /= n_iter\n",
    "        losses['div(v)_loss'][-1] /= n_iter\n",
    "        losses['ns_loss'][-1] /= n_iter\n",
    "        losses['v_inlet_loss'][-1] /= n_iter\n",
    "        losses['p_outlet_loss'][-1] /= n_iter\n",
    "        losses['PINN_loss'][-1] /= n_iter\n",
    "        losses['lr'][-1] /= n_iter\n",
    "\n",
    "        \n",
    "        if losses['PINN_loss'][-1] < best_loss:\n",
    "            best_loss = losses['PINN_loss'][-1]\n",
    "            torch.save(model.state_dict(), f'work/{METHOD}_{SCHEDULER_PATIENCE}.pth')\n",
    "            torch.save(optimizer.state_dict(), f'work/{METHOD}_opt_{SCHEDULER_PATIENCE}.pth')\n",
    "            \n",
    "\n",
    "        if (not i % 1000) and i:\n",
    "            print(f'\\nIteration {i + 1}')\n",
    "            print('res_loss', losses['residual_loss'][-1])\n",
    "            print('div(v)_loss', losses['div(v)_loss'][-1])\n",
    "            print('ns_loss', losses['ns_loss'][-1])\n",
    "            print('v_inlet_loss', losses['v_inlet_loss'][-1])\n",
    "            print('p_outlet_loss', losses['p_outlet_loss'][-1])\n",
    "            print('loss', losses['PINN_loss'][-1])\n",
    "            print('lr', losses['lr'][-1])\n",
    "            with open(f'work/{METHOD}_history_{SCHEDULER_PATIENCE}.json', 'w') as fp:\n",
    "                json.dump(losses, fp)\n",
    "            \n",
    "            \n",
    "        if (not i % SCHEDULER_PATIENCE):\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    del x_interior, inputs_interior, emb_interior, inputs_boundary, mask_\n",
    "    del outputs_interior, outputs_walls, outputs_inlet, outputs_outlet\n",
    "    del dv1, dv2, d2v1, d2v2, dp\n",
    "    del res_loss, div_loss, v0_loss, v_inlet_loss, p_outlet_loss, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TYPE == 'train':\n",
    "    torch.save(model.state_dict(), f'work/{METHOD}_{SCHEDULER_PATIENCE}.pth')\n",
    "    torch.save(optimizer.state_dict(), f'work/{METHOD}_opt_{SCHEDULER_PATIENCE}.pth')\n",
    "    with open(f'work/{METHOD}_history_{SCHEDULER_PATIENCE}.json', 'w') as fp:\n",
    "        json.dump(losses, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {'hidden_layer_size': N_NEURONS, 'hidden_layers': N_LAYERS}\n",
    "\n",
    "model = MLP(hyp).to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'work/{METHOD}_{SCHEDULER_PATIENCE}.pth'))\n",
    "\n",
    "with open(f'work/{METHOD}_history_{SCHEDULER_PATIENCE}.json', 'r') as fp:\n",
    "    losses = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in losses:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.train_history_plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_history_plot(history, mode='Train'):\n",
    "    \"\"\"Plot train history.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dict of lists with train history.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in history:\n",
    "        fig, ax = plt.subplots(figsize=(PARAMS['figsize'] * 2, PARAMS['figsize']))\n",
    "\n",
    "        if mode == 'Train':\n",
    "            ax.plot(history[i], c='r')\n",
    "            ax.set_title(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            ax.set_ylabel('lr' if i == 'lr' else 'Loss')\n",
    "            ax.legend(['Train'])\n",
    "            if min(history[i]) > 0:\n",
    "                ax.set_yscale('log')\n",
    "            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.3)\n",
    "        else:\n",
    "            ax.plot(history[i], c='b')\n",
    "            ax.set_title(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            ax.set_ylabel('lr' if i == 'lr' else 'Loss')\n",
    "            ax.legend(['Val'])\n",
    "            if min(history[i]) > 0:\n",
    "                ax.set_yscale('log')\n",
    "            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis.train_history_plot(val_losses, 'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassDataset('dataset/mini_train',encoder)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "val_dataset = ClassDataset('dataset/mini_val',encoder)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(loader, title):\n",
    "    print(f'{title} visualization')\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for emb_interior, x_interior, inputs_boundary, mask_, center, h in loader:\n",
    "            x1 = torch.linspace(-L1 / 2, L1 / 2, N)\n",
    "            x2 = torch.linspace(-L2 / 2, L2 / 2, N)\n",
    "            x1, x2 = torch.meshgrid((x1, x2))\n",
    "            x = torch.stack((x1.flatten(0), x2.flatten(0)), 1).to(DEVICE)\n",
    "\n",
    "            inputs = torch.cat((emb_interior[0, 0].repeat(1, N * N, 1), x.to(DEVICE).unsqueeze(0)), 2)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            v1 = (outputs[..., 0].detach().cpu().reshape(N, N) * mask_[0])[0]\n",
    "            v2 = (outputs[..., 1].detach().cpu().reshape(N, N) * mask_[0])[0]\n",
    "            p = (outputs[..., 2].detach().cpu().reshape(N, N) * mask_[0])[0]\n",
    "\n",
    "            print(f'ID1: {emb_interior[0][0][0].item()}, ID2: {emb_interior[0][0][1].item()}, weight: {emb_interior[0][0][2].item()}')\n",
    "            print(f'v_abs: {np.sqrt((np.max(v1.numpy()))**2 + (np.max(v2.numpy()))**2)}')\n",
    "            print(f'p_max: {np.max(p.numpy())}')\n",
    "\n",
    "            vis.flow_visualization(v1, v2, x1, x2)\n",
    "            vis.distribution_visualization(v1, 'v1, m/s (PINN)')\n",
    "            vis.distribution_visualization(v2, 'v2, m/s (PINN)')\n",
    "            vis.distribution_visualization(p, 'p, Pa (PINN)')\n",
    "\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(train_loader, 'Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(val_loader, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*train_dataset[2][1].T.cpu(), s=2)\n",
    "plt.scatter(*train_dataset[2][2][:, 2:].T.cpu(), s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = ClassDataset('mini_dataset', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(mini_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,50))\n",
    "i=1\n",
    "for emb_interior, x_interior, inputs_boundary, mask_, center, h in loader:\n",
    "    plt.subplot(11,1,i)\n",
    "    plt.title(f\"Latent Vector ({LATENT_DIM} dim)\")\n",
    "    latent_vector = emb_interior[0].cpu().numpy()\n",
    "    colors = ['r' if v < 0 else 'b' for v in latent_vector[0]]\n",
    "    plt.bar(range(LATENT_DIM), latent_vector[0], color=colors)\n",
    "    plt.xlabel(\"Dimension Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    i+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,250))\n",
    "img_features = np.zeros((128,11))\n",
    "i=0\n",
    "for emb_interior, x_interior, inputs_boundary, mask_, center, h in loader:\n",
    "    latent_vector = emb_interior[0].cpu().numpy()\n",
    "    img_features[:,i] = latent_vector[0]\n",
    "    i+=1\n",
    "\n",
    "for j in range(128):\n",
    "    plt.subplot(128,1,j+1)\n",
    "    plt.title(f\"Feature interpolation (11 img)\")\n",
    "    colors = ['r' if v < 0 else 'b' for v in img_features[j]]\n",
    "    plt.bar(range(11), img_features[j], color=colors)\n",
    "    plt.xlabel(\"image\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    i+=1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
